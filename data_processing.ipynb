{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 数据预处理，分为6各部分。data preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 1: import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 2: importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n",
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "'''此处编译花了一下午时间，尝试了绝对路径的r，以及/和\\\\等方法，后来才发现是文件名后面多添加了一个空格符,如下'''\n",
    "dataset = pd.read_csv('/Users/wangjiabin/Desktop/summary/ML_preprocess/Data.csv')\n",
    "X =dataset.iloc[:,:-1].values\n",
    "print(X)\n",
    "'iloc[:,:-1], :-1代表从右往左所有的值，但是不包含那列值，-1，代表那列值'\n",
    "'country,age,salary,purchased'\n",
    "Y= dataset.iloc[:,3].values\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 3: sloving missing data Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n",
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "'''\n",
    "numeric_features = ['Age','Salary']\n",
    "numeric_transformer =Pipeline(steps=[('imputer',SimpleImputer(missing_values=np.nan,strategy='mean')),\n",
    "                                     ('scaler',StandardScaler())])\n",
    "\n",
    "categorical_features = ['Country','Purchased']\n",
    "categorical_transformer = Pipeline(steps=[('imputer',SimpleImputer(strategy='contant',fill_value ='missing')),\n",
    "                                          ('onehot',OneHotEncoder(handle_unknown='ignore'))])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "numeric_transformer.fit()\n",
    "X = dataset.drop('Purchased', axis=1)\n",
    "y = dataset['Purchased']\n",
    "print(X)\n",
    "print(y)\n",
    "'''\n",
    "'pclass, survived, name, sex, age, sibsp, parch, ticket, fare, cabin, embarked, boat, body, home.dest'\n",
    "\n",
    "\n",
    "imputer =SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "'Imputer实现对nan值的替换，可以替换成平均值，中位值等，axis=0代表列，1代表行，因为其应该有参考坐标'\n",
    "imputer =imputer.fit(X[:,1:3])\n",
    "'对X进行替换，用fit函数，之后再进行transform转换'\n",
    "X[:,1:3] = imputer.transform(X[:,1:3])\n",
    "print(X)\n",
    "print(Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: 1) encoding categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1 2 1 0 2 0 1 0]\n",
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X =LabelEncoder()\n",
    "X[:,0] = labelencoder_X.fit_transform(X[:,0])\n",
    "#X = onehotencoder.fit_transform(X).toarray()\n",
    "#X=np.array(columnTransformer.fit_transform(X),dtype=np.str)\n",
    "print(X[:,0] )\n",
    "print(Y)\n",
    "#X=np.array(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) creat dummy variable\n",
    "#问题1：为什么用dummy：https://blog.csdn.net/u011467621/article/details/48754679\n",
    "#问题2：onehotencoder 和 labelencoder， 两个不同的意义：https://zhuanlan.zhihu.com/p/33569866\n",
    "#问题3：再解释https://medium.com/ai反斗城/preprocessing-data-onehotencoder-labelencoder-實作-968936124d59\n",
    "# 此处https://www.cnblogs.com/zhoukui/p/9159909.html， 讨论fit()+transform() 和fit_transform()两种用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "#ct = ColumnTransformer([(\"Country\", OneHotEncoder(), [1])], remainder = 'passthrough')\n",
    "#X = ct.fit_transform(X).toarray()\n",
    "'''\n",
    "# Country column\n",
    "columnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [0])],     remainder='passthrough')\n",
    "X=np.array(columnTransformer.fit_transform(X),dtype=np.str)\n",
    "'''\n",
    "onehotencoder = OneHotEncoder(sparse = False)\n",
    "X = onehotencoder.fit_transform(X)#.toarray()\n",
    "labelencoder_Y = LabelEncoder()\n",
    "Y = labelencoder_Y.fit_transform(Y)\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 5: spliting the datasets into training sets and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size =0.2, random_state =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 6: Feature scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (23,)\n",
      "y_train (8,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "x_train =sc_X.fit_transform(x_train)\n",
    "x_test = sc_X.fit_transform(x_test)\n",
    "print('x_train',x_train[0,:].shape)\n",
    "print('y_train',y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import matplotlib.pyplot as plt \\nx=np.arange(-4,4)\\nl1=plt.plot(x_train,y_train,'r--')\\nl2=plt.plot(x_test,y_test,'g--')\\nplt.scatter(x_train[0,:],y_train,'ro-',label='train')\\nplt.scatter(x_test,y_test,'g+-',label='test')\\nplt.title('train and test comparison')\\nplt.xlabel('row')\\nplt.ylabel('column')\\nplt.legend()\\nplt.show()\""
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import matplotlib.pyplot as plt \n",
    "x=np.arange(-4,4)\n",
    "l1=plt.plot(x_train,y_train,'r--')\n",
    "l2=plt.plot(x_test,y_test,'g--')\n",
    "plt.scatter(x_train[0,:],y_train,'ro-',label='train')\n",
    "plt.scatter(x_test,y_test,'g+-',label='test')\n",
    "plt.title('train and test comparison')\n",
    "plt.xlabel('row')\n",
    "plt.ylabel('column')\n",
    "plt.legend()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onehotencoder 解释：https://www.cnblogs.com/zhoukui/p/9159909.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 1. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from sklearn.preprocessing import  OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit([[0, 0, 3],\n",
    "         [1, 1, 0],\n",
    "         [0, 2, 1],\n",
    "         [1, 0, 2]])\n",
    "\n",
    "ans = enc.transform([[0, 1, 3]]).toarray()  # 如果不加 toarray() 的话，输出的是稀疏的存储格式，即索引加值的形式，也可以通过参数指定 sparse = False 来达到同样的效果\n",
    "print(ans) # 输出 [[ 1.  0.  0.  1.  0.  0.  0.  0.  1.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-163-f6d0982906c7>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-163-f6d0982906c7>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    [[1. 0. 1. 0. 0. 0. 0. 0. 1.]]\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[[1. 0. 1. 0. 0. 0. 0. 0. 1.]]\n",
    "[[1. 0. 0. 1. 0. 0. 0. 0. 1.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
