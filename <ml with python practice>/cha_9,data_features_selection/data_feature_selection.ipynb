{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we say :data and features decide the upper limition, while model and algorithm are the ways to get close to the upper limition.  \n",
    "the methods to feature processing:  \n",
    "1)data preprocessing  \n",
    "2)feature selection  \n",
    "3)dimension reduction  \n",
    "\n",
    "**In this chapter, it mainly covers four features selection methods**  \n",
    "1) single variable feature selection  \n",
    "2) regression features elimination  \n",
    "3) PCA  \n",
    "4) The importance of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.feature selection**  \n",
    "feature selection is a process, which helps the results of feature data have a higher precision, or find the feature data that we are interested in. if the data contains some unrelated features,the precision will decline, also interfere with prediction to new data, especially in the linear algorithms. so before building the model,feature selection help following:  \n",
    "1) reduce overfitting  \n",
    "2  imporve precision  \n",
    "3) reduce training time  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.single variable feature selection**  \n",
    "**chi-square test** is a method for testing the correlation between independent variable X and dependent variable Y. example: x with N variables, y with M variables. chi-square test can test deviation of statistical samples and theorical samples. higher the value it is, more coincident it is.  \n",
    "SelectKBest:https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n",
      "[ 111.52  1411.887   17.605   53.108 2175.565  127.669    5.393  181.304]\n",
      "[[148.    0.   33.6  50. ]\n",
      " [ 85.    0.   26.6  31. ]\n",
      " [183.    0.   23.3  32. ]\n",
      " ...\n",
      " [121.  112.   26.2  30. ]\n",
      " [126.    0.   30.1  47. ]\n",
      " [ 93.    0.   30.4  23. ]]\n"
     ]
    }
   ],
   "source": [
    "filename ='/Users/wangjiabin/ML_2020_summer/<ml with python practice>/cha_7_data_visibility/pima_data.csv'\n",
    "columns =['Pregnancies', 'Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age','Outcome']\n",
    "dataset = read_csv(filename)\n",
    "array = dataset.values\n",
    "print(array.shape)\n",
    "X =array[:,0:8]\n",
    "Y=array[:,8]\n",
    "test = SelectKBest(score_func = chi2, k =4)# k means top how many features can be selected.\n",
    "fit =test.fit(X,Y)\n",
    "set_printoptions(precision =3)\n",
    "print(fit.scores_)\n",
    "features = fit.transform(X)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.recursive features elimination**  \n",
    "**RFE** Recursive feature elimination USES a base model to conduct multiple rounds of training. After each round of training, the features of several weight coefficients are eliminated, and then the next round of training is conducted based on the new feature set. according to the precision of every base model, finding the one influences most to the final result.  \n",
    "**conclusion of the code:** when setting 3 features, the features in all the given 8 features are selected will be assigned with true and rank with 1  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:\n",
      "3\n",
      "selected features:\n",
      "[ True False False False False  True  True False]\n",
      "rank of features:\n",
      "[1 2 4 5 6 1 1 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangjiabin/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass n_features_to_select=3 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n",
      "/Users/wangjiabin/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 3)\n",
    "fit = rfe.fit(X,Y)\n",
    "print('number of features:')\n",
    "print(fit.n_features_)\n",
    "print('selected features:')\n",
    "print(fit.support_)\n",
    "print('rank of features:')\n",
    "print(fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.PCA**  \n",
    "PCA USES linear algebra to transform and compress data, often called data dimension reduction.  \n",
    "**Comparison PCA&LDA**  \n",
    "1)Pca is for the maximum divergence of mapped samples, while LDA is for the best classification performance of mapped samples  \n",
    "2)PCA is unsupervised(clustering), LDA is supervised.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expalnation of variance:[0.889 0.062 0.026]\n",
      "[[-2.022e-03  9.781e-02  1.609e-02  6.076e-02  9.931e-01  1.401e-02\n",
      "   5.372e-04 -3.565e-03]\n",
      " [-2.265e-02 -9.722e-01 -1.419e-01  5.786e-02  9.463e-02 -4.697e-02\n",
      "  -8.168e-04 -1.402e-01]\n",
      " [-2.246e-02  1.434e-01 -9.225e-01 -3.070e-01  2.098e-02 -1.324e-01\n",
      "  -6.400e-04 -1.255e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components =3)\n",
    "fit =pca.fit(X)\n",
    "print('expalnation of variance:%s'% fit.explained_variance_ratio_)\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.importance of features**  \n",
    "Bagged decision trees, random forest and extreme randon tree are methods to calculate the importance of features. following give the example for calculating the values of importance.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.107 0.238 0.101 0.081 0.073 0.138 0.12  0.142]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = ExtraTreesClassifier()\n",
    "fit = model.fit(X,Y)\n",
    "print(fit.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
