{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "algorithm integration means combine different algorithms together so that we can better result. here we gonna use following algotithms:  \n",
    "1)bagging:将数据集分离成多个子集，然后通过各个子集训练多个模型  \n",
    "2)boosting：训练多个模型，并将模型组成一个序列，序列中的每个模型都会修正前一个模型的错误。  \n",
    "3)voting：训练多个模型，并采用样本统计来提高模型的精准度。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.bagging**  \n",
    "for example: you have sick, go to n hospital and see n doctors, and all the doctors give you prescriptions,after cheacking, the one shown in the description most could be the optimal solution.this is the bagging method how it works.there are three different bagging methods:  \n",
    "1)bagged decision tree  \n",
    "2)random forest  \n",
    "3)extra tree  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 bagged decision tree**  \n",
    "when the data sets have large variance,bagging method is very effecient. most common example is bagging algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 8)\n",
      "(768,)\n",
      "0.7578263841421736\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "filename ='/Users/wangjiabin/ML_2020_summer/<ml with python practice>/cha_7_data_visibility/pima_data.csv'\n",
    "columns =['Pregnancies', 'Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age','Outcome']\n",
    "dataset = read_csv(filename)\n",
    "array = dataset.values\n",
    "#print(array.shape)\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "num_folds = 10\n",
    "seed =7\n",
    "kfold = KFold(n_splits =num_folds,shuffle =True, random_state =seed)\n",
    "cart = DecisionTreeClassifier()\n",
    "num_tree =100\n",
    "model = BaggingClassifier(base_estimator =cart, n_estimators = num_tree, random_state =seed)\n",
    "result = cross_val_score(model, X,Y, cv = kfold)\n",
    "print(result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 Random forest**  \n",
    "random forest means use a random way to build a forest, which consists of many decision tree, also every decision tree has no any relation. when getting random forest, if there is a new sample input, using every decision tree to test from the forest, making sure this sample belong to which class, and which class is selected most, then predict this sample belongs to this class.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.759107997265892\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "max_features = 3\n",
    "model = RandomForestClassifier(n_estimators = num_tree, random_state = seed, max_features = max_features)\n",
    "result = cross_val_score(model, X,Y,cv= kfold)\n",
    "print(result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 Extra tree**  \n",
    "**1.3.1 difference between extra tree and random forest**  \n",
    "1)random forest uses bagging model while extra random forest uses all the train sample to get decision trees,which means every decision tree using same train data.  \n",
    "2)random forest gets optimal bifurcation in a random subsets,while extra random tree absolutely selection bifurcation features.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7630211893369789\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = ExtraTreesClassifier(n_estimators = num_tree, random_state = seed, max_features = max_features)\n",
    "result = cross_val_score(model, X,Y,cv = kfold)\n",
    "print(result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 algorithm improvement: adaboost, stochastic Gradient boosting**  \n",
    "**1)Adaboost**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7552802460697198\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "num_tree = 30\n",
    "model =AdaBoostClassifier(n_estimators =num_tree, random_state =seed)\n",
    "result = cross_val_score(model,X,Y,cv =kfold)\n",
    "print(result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)GBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7604921394395079\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "num_tree = 100\n",
    "model = GradientBoostingClassifier(n_estimators = num_tree, random_state = seed)\n",
    "result = cross_val_score(model,X,Y,cv = kfold)\n",
    "print(result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.voting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7695830485304171\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "models =[]\n",
    "model_logistic = LogisticRegression(solver='lbfgs',max_iter=999)\n",
    "models.append(('Logistic',model_logistic))\n",
    "model_svc = SVC()\n",
    "models.append(('svm',model_svc))\n",
    "model_cart = DecisionTreeClassifier()\n",
    "models.append(('model_cart',model_cart))\n",
    "ensemble_model = VotingClassifier(estimators = models)\n",
    "result =cross_val_score(ensemble_model,X,Y,cv=kfold)\n",
    "print(result.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
